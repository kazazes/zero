{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def clean_ft():\n",
    "  print('Pruning FT...')\n",
    "  preprune_count = 0\n",
    "  postprune_count = 0\n",
    "  for filename in os.listdir(f\"{output_dir}/ft.com\"):\n",
    "      filepath = os.path.join(f\"{output_dir}/ft.com\", filename)\n",
    "      with open(filepath, 'r') as f:\n",
    "          data = [json.loads(line) for line in f]\n",
    "\n",
    "      # regex pattern to match everything after the last http, including the http\n",
    "\n",
    "      pattern = r\"(http[s]?://.*)\"\n",
    "\n",
    "      # iterate over each item in the data\n",
    "      for item in data:\n",
    "          # replace the matched pattern with an empty string\n",
    "          match = re.search(pattern, item['url'])\n",
    "          if match:\n",
    "              item['url'] = match.group(1)\n",
    "      # only include urls with /content/ in them\n",
    "      filtered = [item for item in data if \"/content/\" in item['url']]\n",
    "\n",
    "      preprune_count += len(data)\n",
    "      postprune_count += len(filtered)\n",
    "\n",
    "      # write the filtered data back to the file\n",
    "      with open(filepath, 'w') as f:\n",
    "          for item in filtered:\n",
    "              f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "    print(f\"Preprune count: {preprune_count}\")\n",
    "    print(f\"Postprune count: {postprune_count}\")\n",
    "\n",
    "def clean_drudge():\n",
    "  print('Pruning drudge...')\n",
    "  preprune_count = 0\n",
    "  postprune_count = 0\n",
    "  for filename in os.listdir(f\"{output_dir}/drudgereport.com\"):\n",
    "      filepath = os.path.join(f\"{output_dir}/drudgereport.com\", filename)\n",
    "      with open(filepath, 'r') as f:\n",
    "          data = [json.loads(line) for line in f]\n",
    "\n",
    "      # regex pattern to match 'https://web.archive.org/web/{date as integer}'\n",
    "      pattern = r\"https://web\\.archive\\.org/web/\\d+/\"\n",
    "\n",
    "      # iterate over each item in the data\n",
    "      for item in data:\n",
    "          # replace the matched pattern with an empty string\n",
    "          item['url'] = re.sub(pattern, '', item['url'])\n",
    "      # filter out urls with only one path component\n",
    "      filtered = [item for item in data if len(urlparse(item['url']).path.split('/')) > 2]\n",
    "      # remove internal links to drudge\n",
    "      filtered = [item for item in filtered if \"drudgereport.com\" not in item['url']]\n",
    "      # remove items with less than three words in text\n",
    "      # filtered = [item for item in filtered if len(item['text'].split()) > 2]\n",
    "\n",
    "      preprune_count += len(data)\n",
    "      postprune_count += len(filtered)\n",
    "\n",
    "      # write the filtered data back to the file\n",
    "      with open(filepath, 'w') as f:\n",
    "          for item in filtered:\n",
    "              f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "  print(f\"Preprune count: {preprune_count}\")\n",
    "  print(f\"Postprune count: {postprune_count}\")\n",
    "  print(f\"Pruned {preprune_count - postprune_count} links or {((preprune_count - postprune_count) / preprune_count) * 100:.2f}%\")\n",
    "\n",
    "\n",
    "def clean_bloomberg():\n",
    "  print('Pruning Bloomberg...')\n",
    "  preprune_count = 0\n",
    "  postprune_count = 0\n",
    "  for filename in os.listdir(f\"{output_dir}/bloomberg.com\"):\n",
    "    filepath = os.path.join(f\"{output_dir}/bloomberg.com\", filename)\n",
    "    with open(filepath, 'r') as f:\n",
    "      data = [json.loads(line) for line in f]\n",
    "\n",
    "    # regex pattern to match everything after the last http, including the http\n",
    "\n",
    "    pattern = r\"(http[s]?://.*)\"\n",
    "\n",
    "    # iterate over each item in the data\n",
    "    for item in data:\n",
    "      # replace the matched pattern with an empty string\n",
    "      match = re.search(pattern, item['url'])\n",
    "      if match:\n",
    "        item['url'] = match.group(1)\n",
    "\n",
    "      # trim the text\n",
    "      item['text'] = item['text'].strip()\n",
    "    # only include urls with /news/articles/ in them\n",
    "    filtered = [item for item in data if \"/news/articles/\" in item['url']]\n",
    "    # remove those with text len less than 5\n",
    "    filtered = [item for item in filtered if len(item['text']) > 5]\n",
    "\n",
    "\n",
    "    preprune_count += len(data)\n",
    "    postprune_count += len(filtered)\n",
    "\n",
    "    # write the filtered data back to the file\n",
    "    with open(filepath, 'w') as f:\n",
    "      for item in filtered:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "  print(f\"Preprune count: {preprune_count}\")\n",
    "  print(f\"Postprune count: {postprune_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# clean_drduge()\n",
    "# clean_ft()\n",
    "# clean_bloomberg()\n",
    "\n",
    "# plot_lines_per_day(dates)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
