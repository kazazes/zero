{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First instinct to make money with LLMs is: scrape headlines from a few sources, FT, drudge, bloomberg, scan for company names, get their performance in that day, fine tune _a model to predict the performance of a company based on the news, and then trade on that._\n",
    "\n",
    "Or we could do macro-vibe trading, take the above the fold headlines, line them up with S&P opens and closes, finetune, predict. \n",
    "\n",
    "Okay, so we need a website snapshotter that'll get past bot detections. But we also need training data, so we'll use the wayback machine. Time for some python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install waybackpy beautifulsoup4 retrying python-dotenv tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you _for your help_ GPT. _I'm going to use the wayback machine to get the last year of drudge, every day at 9 AM ET, and 5._ And a proxy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from waybackpy import WaybackMachineCDXServerAPI\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "username = os.getenv('SMARTPROXY_USERNAME')\n",
    "password = os.getenv('SMARTPROXY_PASSWORD')\n",
    "proxy = f\"https://{username}:{password}@gate.smartproxy.com:7000\"\n",
    "\n",
    "proxies = {'https': proxy}\n",
    "user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\"\n",
    "headers = {'User-Agent': user_agent}\n",
    "\n",
    "def play_success_sound():\n",
    "    os.system('afplay /System/Library/Sounds/Pop.aiff')\n",
    "\n",
    "def get_links_at_date(url, year, month, day, hour, minute):\n",
    "    w = WaybackMachineCDXServerAPI(url, user_agent=user_agent)\n",
    "    url = w.near(year=year, month=month, day=day, hour=hour, minute=minute).archive_url\n",
    "    response = requests.get(url, headers=headers, proxies=proxies)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    links = soup.find_all('a')\n",
    "    article_links = [(link.get('href'), link.text) for link in links]\n",
    "    article_links = [link for link in article_links if link[1]]\n",
    "    return article_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from retrying import retry\n",
    "\n",
    "def scrape_drudge_with_proxy(dates):\n",
    "    for date in tqdm(dates, desc=\"Drudge progress\", unit=\"date\"):\n",
    "        # check if we already have the data\n",
    "        if os.path.exists(f\"01-some-data/drudgereport.com/{date}.jsonl\"):\n",
    "            continue\n",
    "        data = []\n",
    "        am_links = get_links_at_date(\"https://drudgereport.com/\", year=date.year, month=date.month, day=date.day, hour=6, minute=0)\n",
    "        for url, text in am_links:\n",
    "            data.append({\"date\": date.isoformat(), \"url\": url, \"text\": text, \"isMorning\": True})\n",
    "        pm_links = get_links_at_date(\"https://drudgereport.com/\", year=date.year, month=date.month, day=date.day, hour=18, minute=0)\n",
    "        for url, text in pm_links:\n",
    "            data.append({\"date\": date.isoformat(), \"url\": url, \"text\": text, \"isMorning\": False})\n",
    "        # write to jsonl\n",
    "        with open(f\"01-some-data/drudgereport.com/{date}.jsonl\", \"w\") as f:\n",
    "            for row in data:\n",
    "                f.write(json.dumps(row) + \"\\n\")\n",
    "        play_success_sound()\n",
    "\n",
    "@retry(wait_exponential_multiplier=1000, wait_exponential_max=60000)\n",
    "def scrape_drudge_with_proxy_retry(dates):\n",
    "    scrape_drudge_with_proxy(dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping is a muddy art, let's let this baby run with some backoff retry logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same thing for the FT. I should probably generalize this to a scraper class, but it's python baby, _we're all about the duct tape._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mkdir 01-some-data/ft.com\n",
    "if not os.path.exists(\"01-some-data/ft.com\"):\n",
    "    os.mkdir(\"01-some-data/ft.com\")\n",
    "\n",
    "def scrape_ft_with_proxy(dates):\n",
    "    for date in tqdm(dates, desc=\"FT progress\", unit=\"date\"):\n",
    "        # check if we already have the data\n",
    "        if os.path.exists(f\"01-some-data/ft.com/{date}.jsonl\"):\n",
    "            continue\n",
    "        data = []\n",
    "        am_links = get_links_at_date(\"https://ft.com/\", year=date.year, month=date.month, day=date.day, hour=6, minute=0)\n",
    "        for url, text in am_links:\n",
    "            data.append({\"date\": date.isoformat(), \"url\": url, \"text\": text, \"isMorning\": True})\n",
    "        pm_links = get_links_at_date(\"https://ft.com/\", year=date.year, month=date.month, day=date.day, hour=18, minute=0)\n",
    "        for url, text in pm_links:\n",
    "            data.append({\"date\": date.isoformat(), \"url\": url, \"text\": text, \"isMorning\": False})\n",
    "        # write to jsonl\n",
    "        with open(f\"01-some-data/ft.com/{date}.jsonl\", \"w\") as f:\n",
    "            for row in data:\n",
    "                f.write(json.dumps(row) + \"\\n\")\n",
    "        os.system('afplay /System/Library/Sounds/Pop.aiff')\n",
    "\n",
    "@retry(wait_exponential_multiplier=1000, wait_exponential_max=60000)\n",
    "def scrape_ft_with_proxy_retry(dates):\n",
    "    scrape_ft_with_proxy(dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally Bloomberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mkdir 01-some-data/bloomberg.com\n",
    "if not os.path.exists(\"01-some-data/bloomberg.com\"):\n",
    "  os.mkdir(\"01-some-data/bloomberg.com\")\n",
    "\n",
    "def scrape_bloomberg_with_proxy(dates):\n",
    "  for date in tqdm(dates, desc=\"Bloomberg progress\", unit=\"date\"):\n",
    "    # check if we already have the data\n",
    "    if os.path.exists(f\"01-some-data/bloomberg.com/{date}.jsonl\"):\n",
    "      continue\n",
    "    data = []\n",
    "    am_links = get_links_at_date(\"https://www.bloomberg.com/markets\", year=date.year, month=date.month, day=date.day, hour=6, minute=0)\n",
    "    for url, text in am_links:\n",
    "      data.append({\"date\": date.isoformat(), \"url\": url, \"text\": text, \"isMorning\": True})\n",
    "    pm_links = get_links_at_date(\"https://www.bloomberg.com/markets\", year=date.year, month=date.month, day=date.day, hour=18, minute=0)\n",
    "    for url, text in pm_links:\n",
    "      data.append({\"date\": date.isoformat(), \"url\": url, \"text\": text, \"isMorning\": False})\n",
    "    # write to jsonl\n",
    "    with open(f\"01-some-data/bloomberg.com/{date}.jsonl\", \"w\") as f:\n",
    "      for row in data:\n",
    "        f.write(json.dumps(row) + \"\\n\")\n",
    "    os.system('afplay /System/Library/Sounds/Pop.aiff')\n",
    "\n",
    "@retry(wait_exponential_multiplier=1000, wait_exponential_max=60000)\n",
    "def scrape_bloomberg_with_proxy_retry(dates):\n",
    "  scrape_bloomberg_with_proxy(dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They all do their thing, now I'll let this puppy run over _night and see what we get._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Drudge progress:   0%|          | 0/1520 [00:00<?, ?date/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Drudge progress:   1%|          | 17/1520 [01:05<1:36:03,  3.83s/date]\n",
      "Drudge progress:   1%|▏         | 21/1520 [01:24<1:40:42,  4.03s/date]\n",
      "Drudge progress:   7%|▋         | 101/1520 [23:47<5:34:20, 14.14s/date]\n",
      "Drudge progress:   8%|▊         | 118/1520 [05:52<7:47:38, 20.01s/date]"
     ]
    }
   ],
   "source": [
    "from retrying import retry\n",
    "\n",
    "@retry(wait_exponential_multiplier=1000, wait_exponential_max=60000)\n",
    "def run_all_scrapers(dates):\n",
    "  # Scrape drudgereport.com\n",
    "  try:\n",
    "    scrape_drudge_with_proxy_retry(dates)\n",
    "  except Exception as e:\n",
    "    print(\"An error occurred while scraping drudgereport.com:\", str(e))\n",
    "\n",
    "  # Scrape ft.com\n",
    "  try:\n",
    "    scrape_ft_with_proxy_retry(dates)\n",
    "  except Exception as e:\n",
    "    print(\"An error occurred while scraping ft.com:\", str(e))\n",
    "\n",
    "  # Scrape bloomberg.com\n",
    "  try:\n",
    "    scrape_bloomberg_with_proxy_retry(dates)\n",
    "  except Exception as e:\n",
    "    print(\"An error occurred while scraping bloomberg.com:\", str(e))\n",
    "\n",
    "now = pd.Timestamp.now()\n",
    "dates = pd.date_range(start=\"2020-01-01\", end=now, freq=\"D\")\n",
    "\n",
    "try:\n",
    "  run_all_scrapers(dates)\n",
    "  os.system('say \"All scrapers have finished running.\"')\n",
    "except Exception as e:\n",
    "  print(\"An error occurred while running all scrapers:\", str(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lovely. But there are a lot of non-news item links in the scrapes. Let's clean up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def clean_ft():\n",
    "  print('Pruning FT...')\n",
    "  preprune_count = 0\n",
    "  postprune_count = 0\n",
    "  for filename in os.listdir(\"01-some-data/ft.com\"):\n",
    "      filepath = os.path.join(\"01-some-data/ft.com\", filename)\n",
    "      with open(filepath, 'r') as f:\n",
    "          data = [json.loads(line) for line in f]\n",
    "\n",
    "      # regex pattern to match everything after the last http, including the http\n",
    "\n",
    "      pattern = r\"(http[s]?://.*)\"\n",
    "\n",
    "      # iterate over each item in the data\n",
    "      for item in data:\n",
    "          # replace the matched pattern with an empty string\n",
    "          match = re.search(pattern, item['url'])\n",
    "          if match:\n",
    "              item['url'] = match.group(1)\n",
    "      # only include urls with /content/ in them\n",
    "      filtered = [item for item in data if \"/content/\" in item['url']]\n",
    "\n",
    "      preprune_count += len(data)\n",
    "      postprune_count += len(filtered)\n",
    "\n",
    "      # write the filtered data back to the file\n",
    "      with open(filepath, 'w') as f:\n",
    "          for item in filtered:\n",
    "              f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "    print(f\"Preprune count: {preprune_count}\")\n",
    "    print(f\"Postprune count: {postprune_count}\")\n",
    "\n",
    "def clean_drudge():\n",
    "  print('Pruning drudge...')\n",
    "  preprune_count = 0\n",
    "  postprune_count = 0\n",
    "  for filename in os.listdir(\"01-some-data/drudgereport.com\"):\n",
    "      filepath = os.path.join(\"01-some-data/drudgereport.com\", filename)\n",
    "      with open(filepath, 'r') as f:\n",
    "          data = [json.loads(line) for line in f]\n",
    "\n",
    "      # regex pattern to match 'https://web.archive.org/web/{date as integer}'\n",
    "      pattern = r\"https://web\\.archive\\.org/web/\\d+/\"\n",
    "\n",
    "      # iterate over each item in the data\n",
    "      for item in data:\n",
    "          # replace the matched pattern with an empty string\n",
    "          item['url'] = re.sub(pattern, '', item['url'])\n",
    "      # filter out urls with only one path component\n",
    "      filtered = [item for item in data if len(urlparse(item['url']).path.split('/')) > 2]\n",
    "      # remove internal links to drudge\n",
    "      filtered = [item for item in filtered if \"drudgereport.com\" not in item['url']]\n",
    "      # remove items with less than three words in text\n",
    "      # filtered = [item for item in filtered if len(item['text'].split()) > 2]\n",
    "\n",
    "      preprune_count += len(data)\n",
    "      postprune_count += len(filtered)\n",
    "\n",
    "      # write the filtered data back to the file\n",
    "      with open(filepath, 'w') as f:\n",
    "          for item in filtered:\n",
    "              f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "  print(f\"Preprune count: {preprune_count}\")\n",
    "  print(f\"Postprune count: {postprune_count}\")\n",
    "  print(f\"Pruned {preprune_count - postprune_count} links or {((preprune_count - postprune_count) / preprune_count) * 100:.2f}%\")\n",
    "\n",
    "\n",
    "def clean_bloomberg():\n",
    "  print('Pruning Bloomberg...')\n",
    "  preprune_count = 0\n",
    "  postprune_count = 0\n",
    "  for filename in os.listdir(\"01-some-data/bloomberg.com\"):\n",
    "    filepath = os.path.join(\"01-some-data/bloomberg.com\", filename)\n",
    "    with open(filepath, 'r') as f:\n",
    "      data = [json.loads(line) for line in f]\n",
    "\n",
    "    # regex pattern to match everything after the last http, including the http\n",
    "\n",
    "    pattern = r\"(http[s]?://.*)\"\n",
    "\n",
    "    # iterate over each item in the data\n",
    "    for item in data:\n",
    "      # replace the matched pattern with an empty string\n",
    "      match = re.search(pattern, item['url'])\n",
    "      if match:\n",
    "        item['url'] = match.group(1)\n",
    "\n",
    "      # trim the text\n",
    "      item['text'] = item['text'].strip()\n",
    "    # only include urls with /news/articles/ in them\n",
    "    filtered = [item for item in data if \"/news/articles/\" in item['url']]\n",
    "    # remove those with text len less than 5\n",
    "    filtered = [item for item in filtered if len(item['text']) > 5]\n",
    "\n",
    "\n",
    "    preprune_count += len(data)\n",
    "    postprune_count += len(filtered)\n",
    "\n",
    "    # write the filtered data back to the file\n",
    "    with open(filepath, 'w') as f:\n",
    "      for item in filtered:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "  print(f\"Preprune count: {preprune_count}\")\n",
    "  print(f\"Postprune count: {postprune_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clean_drduge()\n",
    "clean_ft()\n",
    "clean_bloomberg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got what we need for now, on to bigger things."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
