{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First instinct to make money with LLMs is: scrape headlines from a few sources, FT, drudge, bloomberg, scan for company names, get their performance in that day, fine tune _a model to predict the performance of a company based on the news, and then trade on that._\n",
    "\n",
    "Or we could do macro-vibe trading, take the above the fold headlines, line them up with S&P opens and closes, finetune, predict. \n",
    "\n",
    "Okay, so we need a website snapshotter that'll get past bot detections. But we also need training data, so we'll use the wayback machine. Time for some python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install waybackpy beautifulsoup4 retrying python-dotenv tqdm matplotlib jupyter ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you _for your help_ GPT. _I'm going to use the wayback machine to get the last year of drudge, every day at 9 AM ET, and 5._ And a proxy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.notebook import tqdm\n",
    "from waybackpy import WaybackMachineCDXServerAPI\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "username = os.getenv('SMARTPROXY_USERNAME')\n",
    "password = os.getenv('SMARTPROXY_PASSWORD')\n",
    "proxy = f\"https://{username}:{password}@gate.smartproxy.com:7000\"\n",
    "\n",
    "proxies = {'https': proxy}\n",
    "user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\"\n",
    "headers = {'User-Agent': user_agent}\n",
    "\n",
    "output_dir = \"01-some-data\"\n",
    "\n",
    "def get_links_at_date(url, year, month, day, hour, minute):\n",
    "    w = WaybackMachineCDXServerAPI(url, user_agent=user_agent)\n",
    "    url = w.near(year=year, month=month, day=day, hour=hour, minute=minute).archive_url\n",
    "    response = requests.get(url, headers=headers, proxies=proxies)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    links = soup.find_all('a')\n",
    "    article_links = [(link.get('href'), link.text) for link in links]\n",
    "    article_links = [link for link in article_links if link[1]]\n",
    "    return article_links\n",
    "\n",
    "def dates_to_scrape():\n",
    "    now = pd.Timestamp.now()\n",
    "    dates = pd.date_range(start=\"2020-01-01\", end=now, freq=\"D\")\n",
    "    return dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from retrying import retry\n",
    "from src.utils import timeout, TimeoutError, play_sound_or_say\n",
    "\n",
    "@timeout(seconds=45, error_message=\"Scraping Drudge timed out\")\n",
    "def scrape_drudge_with_proxy(dates, progress_bar):\n",
    "    for date in dates:\n",
    "        if os.path.exists(f\"{output_dir}/drudgereport.com/{date}.jsonl\"):\n",
    "            continue\n",
    "        data = []\n",
    "        am_links = get_links_at_date(\"https://drudgereport.com/\", year=date.year, month=date.month, day=date.day, hour=6, minute=0)\n",
    "        for url, text in am_links:\n",
    "            data.append({\"date\": date.isoformat(), \"url\": url, \"text\": text, \"isMorning\": True})\n",
    "        pm_links = get_links_at_date(\"https://drudgereport.com/\", year=date.year, month=date.month, day=date.day, hour=18, minute=0)\n",
    "        for url, text in pm_links:\n",
    "            data.append({\"date\": date.isoformat(), \"url\": url, \"text\": text, \"isMorning\": False})\n",
    "        # write to jsonl\n",
    "        with open(f\"{output_dir}/drudgereport.com/{date}.jsonl\", \"w\") as f:\n",
    "            for row in data:\n",
    "                f.write(json.dumps(row) + \"\\n\")\n",
    "        progress_bar.update(1)\n",
    "        play_sound_or_say(\"d\")\n",
    "\n",
    "@retry(wait_exponential_multiplier=1000, wait_exponential_max=60000)\n",
    "def scrape_drudge_with_proxy_retry(dates, progress_bar):\n",
    "    scrape_drudge_with_proxy(dates, progress_bar)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same thing for the FT. I should probably generalize this to a scraper class, but it's python baby, _we're all about the duct tape._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeout(seconds=45, error_message=\"Scraping FT timed out\")\n",
    "def scrape_ft_on_date(date):\n",
    "    data = []\n",
    "    am_links = get_links_at_date(\"https://ft.com/\", year=date.year, month=date.month, day=date.day, hour=6, minute=0)\n",
    "    for url, text in am_links:\n",
    "        data.append({\"date\": date.isoformat(), \"url\": url, \"text\": text, \"isMorning\": True})\n",
    "    pm_links = get_links_at_date(\"https://ft.com/\", year=date.year, month=date.month, day=date.day, hour=18, minute=0)\n",
    "    for url, text in pm_links:\n",
    "        data.append({\"date\": date.isoformat(), \"url\": url, \"text\": text, \"isMorning\": False})\n",
    "    # write to jsonl\n",
    "    with open(f\"{output_dir}/ft.com/{date}.jsonl\", \"w\") as f:\n",
    "        for row in data:\n",
    "            f.write(json.dumps(row) + \"\\n\")\n",
    "\n",
    "def scrape_ft_with_proxy(dates, progress_bar):\n",
    "    if not os.path.exists(f\"{output_dir}/ft.com\"):\n",
    "        os.mkdir(f\"{output_dir}/ft.com\")\n",
    "    for date in dates:\n",
    "        if os.path.exists(f\"{output_dir}/ft.com/{date}.jsonl\"):\n",
    "            continue\n",
    "        scrape_ft_on_date(date)\n",
    "        progress_bar.update(1)\n",
    "        play_sound_or_say(\"f\")\n",
    "\n",
    "@retry(wait_exponential_multiplier=1000, wait_exponential_max=60000)\n",
    "def scrape_ft_with_proxy_retry(dates, progress_bar):\n",
    "    scrape_ft_with_proxy(dates, progress_bar)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally Bloomberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeout(seconds=90, error_message=\"Scraping Bloomberg timed out\")\n",
    "def scrape_bloomberg_with_proxy(dates, progress_bar):\n",
    "  if not os.path.exists(f\"{output_dir}/bloomberg.com\"):\n",
    "    os.mkdir(f\"{output_dir}/bloomberg.com\")\n",
    "  for date in dates:\n",
    "    if os.path.exists(f\"{output_dir}/bloomberg.com/{date}.jsonl\"):\n",
    "      continue\n",
    "    data = []\n",
    "    am_links = get_links_at_date(\"https://www.bloomberg.com/markets\", year=date.year, month=date.month, day=date.day, hour=6, minute=0)\n",
    "    for url, text in am_links:\n",
    "      data.append({\"date\": date.isoformat(), \"url\": url, \"text\": text, \"isMorning\": True})\n",
    "    pm_links = get_links_at_date(\"https://www.bloomberg.com/markets\", year=date.year, month=date.month, day=date.day, hour=18, minute=0)\n",
    "    for url, text in pm_links:\n",
    "      data.append({\"date\": date.isoformat(), \"url\": url, \"text\": text, \"isMorning\": False})\n",
    "    # write to jsonl\n",
    "    with open(f\"{output_dir}/bloomberg.com/{date}.jsonl\", \"w\") as f:\n",
    "      for row in data:\n",
    "        f.write(json.dumps(row) + \"\\n\")\n",
    "    progress_bar.update(1)\n",
    "    play_sound_or_say(\"b\")\n",
    "    \n",
    "\n",
    "@retry(wait_exponential_multiplier=1000, wait_exponential_max=60000)\n",
    "def scrape_bloomberg_with_proxy_retry(dates, progress_bar):\n",
    "  scrape_bloomberg_with_proxy(dates, progress_bar)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's track scraping progress with a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def count_lines_per_file(directory, date_range):\n",
    "    lines_per_file = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        with open(filepath, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            date = filename.split()[0]  # Assuming the date is the first part of the filename\n",
    "            if date in date_range:\n",
    "                lines_per_file[date] = len(lines)\n",
    "    return lines_per_file\n",
    "\n",
    "def plot_lines_per_day(dates, ax=None):\n",
    "    directories = [f\"{output_dir}/drudgereport.com\", f\"{output_dir}/ft.com\", f\"{output_dir}/bloomberg.com\"]\n",
    "    labels = [\"Drudge Report\", \"Financial Times\", \"Bloomberg\"]\n",
    "    \n",
    "    if ax is None:\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        ax = plt.gca()  # Get the current axes, creating them if necessary\n",
    "    \n",
    "    date_range = pd.date_range(start=dates[0], end=dates[-1], freq='D').strftime('%Y-%m-%d').tolist()\n",
    "    first_of_month = [date for date in date_range if date.endswith('-01')]\n",
    "    \n",
    "    missing_counts = {}\n",
    "    \n",
    "    for directory, label in zip(directories, labels):\n",
    "        lines_per_file = count_lines_per_file(directory, date_range)\n",
    "        all_dates = sorted(date_range)\n",
    "        counts = [lines_per_file[date] if date in lines_per_file else 0 for date in all_dates]\n",
    "        \n",
    "        missing_counts[label] = sum(1 for count in counts if count == 0)\n",
    "        \n",
    "        ax.plot(all_dates, counts, label=f\"{label} (Missing: {missing_counts[label]})\")\n",
    "    \n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.set_ylabel(\"Number of Links (Line Count)\")\n",
    "    ax.set_title(\"Links per Day for Each Dataset (Based on Line Count)\")\n",
    "    ax.set_xticks(first_of_month)\n",
    "    ax.set_xticklabels(first_of_month, rotation=45)\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "plot_lines_per_day(dates_to_scrape())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let this puppy run over _night and see what we get._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from IPython.display import display, clear_output, update_display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def run_all_scrapers(dates, progress_bar):\n",
    "    # Initialize a stop event\n",
    "    stop_event = threading.Event()\n",
    "\n",
    "    # Start the plot updater thread\n",
    "    plot_thread = threading.Thread(target=update_plot_every_5_seconds, args=(dates, stop_event))\n",
    "    plot_thread.start()\n",
    "\n",
    "    # Scrape drudgereport.com\n",
    "    try:\n",
    "        scrape_drudge_with_proxy_retry(dates, progress_bar)\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred while scraping drudgereport.com:\", str(e))\n",
    "\n",
    "    # Scrape bloomberg.com\n",
    "    try:\n",
    "        scrape_bloomberg_with_proxy_retry(dates, progress_bar)\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred while scraping bloomberg.com:\", str(e))\n",
    "\n",
    "    # Scrape ft.com\n",
    "    try:\n",
    "        scrape_ft_with_proxy_retry(dates, progress_bar)\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred while scraping ft.com:\", str(e))\n",
    "\n",
    "    # Once scraping is done, stop the plot updater thread\n",
    "    stop_event.set()\n",
    "    plot_thread.join()  # Wait for the plot updater thread to finish\n",
    "\n",
    "def update_plot_every_5_seconds(dates, stop_event):\n",
    "    fig, ax = plt.subplots(figsize=(14, 7))\n",
    "    display_handle = display(fig, display_id=True)  # Create a display handle for the figure\n",
    "\n",
    "    while not stop_event.is_set():\n",
    "        ax.clear()  # Clear the current plot\n",
    "        plot_lines_per_day(dates, ax=ax)  # Update the plot with the current data\n",
    "        update_display(fig, display_id=display_handle.display_id)  # Update the existing figure display\n",
    "        time.sleep(5)  # Wait for 5 seconds before updating again\n",
    "\n",
    "    plt.close(fig)  # Close the figure when done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might take a few days..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the progress bar initial value\n",
    "def scrape_count_existing():\n",
    "    return len(os.listdir(f\"{output_dir}/drudgereport.com\")) + len(os.listdir(f\"{output_dir}/ft.com\")) + len(os.listdir(f\"{output_dir}/bloomberg.com\"))\n",
    "\n",
    "dates = dates_to_scrape()\n",
    "progress_bar = tqdm(total=len(dates) * 3, initial=scrape_count_existing(), desc=\"Overall scraping progress\")\n",
    "run_all_scrapers(dates, progress_bar)\n",
    "progress_bar.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lovely. But there are a lot of non-news item links in the scrapes. Let's clean up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def clean_ft():\n",
    "  print('Pruning FT...')\n",
    "  preprune_count = 0\n",
    "  postprune_count = 0\n",
    "  for filename in os.listdir(f\"{output_dir}/ft.com\"):\n",
    "      filepath = os.path.join(f\"{output_dir}/ft.com\", filename)\n",
    "      with open(filepath, 'r') as f:\n",
    "          data = [json.loads(line) for line in f]\n",
    "\n",
    "      # regex pattern to match everything after the last http, including the http\n",
    "\n",
    "      pattern = r\"(http[s]?://.*)\"\n",
    "\n",
    "      # iterate over each item in the data\n",
    "      for item in data:\n",
    "          # replace the matched pattern with an empty string\n",
    "          match = re.search(pattern, item['url'])\n",
    "          if match:\n",
    "              item['url'] = match.group(1)\n",
    "      # only include urls with /content/ in them\n",
    "      filtered = [item for item in data if \"/content/\" in item['url']]\n",
    "\n",
    "      preprune_count += len(data)\n",
    "      postprune_count += len(filtered)\n",
    "\n",
    "      # write the filtered data back to the file\n",
    "      with open(filepath, 'w') as f:\n",
    "          for item in filtered:\n",
    "              f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "    print(f\"Preprune count: {preprune_count}\")\n",
    "    print(f\"Postprune count: {postprune_count}\")\n",
    "\n",
    "def clean_drudge():\n",
    "  print('Pruning drudge...')\n",
    "  preprune_count = 0\n",
    "  postprune_count = 0\n",
    "  for filename in os.listdir(f\"{output_dir}/drudgereport.com\"):\n",
    "      filepath = os.path.join(f\"{output_dir}/drudgereport.com\", filename)\n",
    "      with open(filepath, 'r') as f:\n",
    "          data = [json.loads(line) for line in f]\n",
    "\n",
    "      # regex pattern to match 'https://web.archive.org/web/{date as integer}'\n",
    "      pattern = r\"https://web\\.archive\\.org/web/\\d+/\"\n",
    "\n",
    "      # iterate over each item in the data\n",
    "      for item in data:\n",
    "          # replace the matched pattern with an empty string\n",
    "          item['url'] = re.sub(pattern, '', item['url'])\n",
    "      # filter out urls with only one path component\n",
    "      filtered = [item for item in data if len(urlparse(item['url']).path.split('/')) > 2]\n",
    "      # remove internal links to drudge\n",
    "      filtered = [item for item in filtered if \"drudgereport.com\" not in item['url']]\n",
    "      # remove items with less than three words in text\n",
    "      # filtered = [item for item in filtered if len(item['text'].split()) > 2]\n",
    "\n",
    "      preprune_count += len(data)\n",
    "      postprune_count += len(filtered)\n",
    "\n",
    "      # write the filtered data back to the file\n",
    "      with open(filepath, 'w') as f:\n",
    "          for item in filtered:\n",
    "              f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "  print(f\"Preprune count: {preprune_count}\")\n",
    "  print(f\"Postprune count: {postprune_count}\")\n",
    "  print(f\"Pruned {preprune_count - postprune_count} links or {((preprune_count - postprune_count) / preprune_count) * 100:.2f}%\")\n",
    "\n",
    "\n",
    "def clean_bloomberg():\n",
    "  print('Pruning Bloomberg...')\n",
    "  preprune_count = 0\n",
    "  postprune_count = 0\n",
    "  for filename in os.listdir(f\"{output_dir}/bloomberg.com\"):\n",
    "    filepath = os.path.join(f\"{output_dir}/bloomberg.com\", filename)\n",
    "    with open(filepath, 'r') as f:\n",
    "      data = [json.loads(line) for line in f]\n",
    "\n",
    "    # regex pattern to match everything after the last http, including the http\n",
    "\n",
    "    pattern = r\"(http[s]?://.*)\"\n",
    "\n",
    "    # iterate over each item in the data\n",
    "    for item in data:\n",
    "      # replace the matched pattern with an empty string\n",
    "      match = re.search(pattern, item['url'])\n",
    "      if match:\n",
    "        item['url'] = match.group(1)\n",
    "\n",
    "      # trim the text\n",
    "      item['text'] = item['text'].strip()\n",
    "    # only include urls with /news/articles/ in them\n",
    "    filtered = [item for item in data if \"/news/articles/\" in item['url']]\n",
    "    # remove those with text len less than 5\n",
    "    filtered = [item for item in filtered if len(item['text']) > 5]\n",
    "\n",
    "\n",
    "    preprune_count += len(data)\n",
    "    postprune_count += len(filtered)\n",
    "\n",
    "    # write the filtered data back to the file\n",
    "    with open(filepath, 'w') as f:\n",
    "      for item in filtered:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "  print(f\"Preprune count: {preprune_count}\")\n",
    "  print(f\"Postprune count: {postprune_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# clean_drduge()\n",
    "# clean_ft()\n",
    "# clean_bloomberg()\n",
    "\n",
    "# plot_lines_per_day(dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got what we need for now, on to bigger things."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
